"""
Advanced AI Routes
Synthetic Data Generation, Data Chat, and Multi-Modal Learning
"""
from flask import Blueprint, request, jsonify
from flask_jwt_extended import jwt_required, get_jwt_identity
from app import db
from app.models.dataset import Dataset
import os
import json
import re

advanced_bp = Blueprint('advanced', __name__)


@advanced_bp.route('/datasets/<int:dataset_id>/chat', methods=['POST'])
@jwt_required()
def chat_with_dataset(dataset_id):
    """
    Chat with Dataset - AI-powered data analysis
    Uses LLM to understand and answer any question about the data
    """
    user_id = int(get_jwt_identity())
    dataset = Dataset.query.filter_by(id=dataset_id, user_id=user_id).first()
    
    if not dataset:
        return jsonify({'error': 'Dataset not found'}), 404
    
    data = request.get_json()
    question = data.get('question', '')
    
    if not question:
        return jsonify({'error': 'No question provided'}), 400
    
    try:
        # Load dataset for analysis
        import pandas as pd
        import numpy as np
        import io
        from app.services.minio_service import get_minio_service
        
        minio_service = get_minio_service()
        file_content = minio_service.download_bytes('datasets', dataset.file_path)
        
        if dataset.file_type == 'csv':
            df = pd.read_csv(io.BytesIO(file_content))
        else:
            df = pd.read_excel(io.BytesIO(file_content))
        
        # Build comprehensive data context
        context = build_comprehensive_context(df)
        
        # Try LLM-based analysis first
        answer, insights = analyze_with_smart_llm(df, question, context)
        
        return jsonify({
            'success': True,
            'answer': answer,
            'insights': insights,
            'visualization': None
        }), 200
        
    except Exception as e:
        print(f"Chat analysis error: {e}")
        import traceback
        traceback.print_exc()
        # Fallback: provide basic analysis
        answer = generate_basic_analysis(dataset, question)
        return jsonify({
            'success': True,
            'answer': answer,
            'insights': [],
            'visualization': None
        }), 200


def build_comprehensive_context(df):
    """Build comprehensive context about the dataframe for LLM"""
    import pandas as pd
    import numpy as np
    
    context = {
        'shape': {'rows': len(df), 'columns': len(df.columns)},
        'columns': [],
        'sample_data': [],
        'statistics': {}
    }
    
    # Detailed column info
    for col in df.columns:
        col_info = {
            'name': col,
            'dtype': str(df[col].dtype),
            'null_count': int(df[col].isna().sum()),
            'unique_count': int(df[col].nunique())
        }
        
        # Add statistics for numeric columns
        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
            col_info['min'] = float(df[col].min()) if not pd.isna(df[col].min()) else None
            col_info['max'] = float(df[col].max()) if not pd.isna(df[col].max()) else None
            col_info['mean'] = round(float(df[col].mean()), 2) if not pd.isna(df[col].mean()) else None
            col_info['median'] = round(float(df[col].median()), 2) if not pd.isna(df[col].median()) else None
            col_info['std'] = round(float(df[col].std()), 2) if not pd.isna(df[col].std()) else None
            col_info['sum'] = round(float(df[col].sum()), 2)
        else:
            # For categorical columns, show top values
            top_values = df[col].value_counts().head(10)
            col_info['top_values'] = {str(k): int(v) for k, v in top_values.items()}
        
        context['columns'].append(col_info)
    
    # Add sample rows (first 10 and random 5)
    sample_df = df.head(10)
    context['sample_data'] = sample_df.to_dict('records')
    
    # Add category-wise aggregations if possible
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    num_cols = df.select_dtypes(include=['number']).columns.tolist()
    
    if cat_cols and num_cols:
        context['aggregations'] = {}
        for cat_col in cat_cols[:2]:  # Top 2 categorical columns
            for num_col in num_cols[:2]:  # Top 2 numeric columns
                try:
                    agg = df.groupby(cat_col)[num_col].agg(['sum', 'mean', 'count']).head(10)
                    context['aggregations'][f'{cat_col}_by_{num_col}'] = agg.to_dict()
                except:
                    pass
    
    return context


def analyze_with_smart_llm(df, question, context):
    """Use LLM with comprehensive context to answer any question"""
    import pandas as pd
    import numpy as np
    
    try:
        import google.generativeai as genai
        
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise Exception("No API key - falling back to statistical analysis")
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Build a detailed prompt with all data context
        columns_info = ""
        for col in context['columns']:
            columns_info += f"\n**{col['name']}** ({col['dtype']}):\n"
            columns_info += f"  - Unique values: {col['unique_count']}, Nulls: {col['null_count']}\n"
            if 'min' in col:
                columns_info += f"  - Stats: min={col['min']}, max={col['max']}, mean={col['mean']}, median={col['median']}, sum={col['sum']}\n"
            if 'top_values' in col:
                top_vals = list(col['top_values'].items())[:5]
                columns_info += f"  - Top values: {', '.join([f'{k}({v})' for k,v in top_vals])}\n"
        
        # Sample data as table
        sample_rows = context['sample_data'][:5]
        sample_table = "| " + " | ".join(df.columns[:8]) + " |\n"
        sample_table += "|" + "|".join(["---"] * min(8, len(df.columns))) + "|\n"
        for row in sample_rows:
            row_vals = [str(row.get(k, ''))[:20] for k in list(df.columns)[:8]]
            sample_table += "| " + " | ".join(row_vals) + " |\n"
        
        prompt = f"""You are an expert data analyst. Analyze this dataset and answer the user's question with specific data insights.

## DATASET OVERVIEW
- **Total Rows:** {context['shape']['rows']:,}
- **Total Columns:** {context['shape']['columns']}

## COLUMN DETAILS
{columns_info}

## SAMPLE DATA (First 5 rows)
{sample_table}

## AGGREGATIONS (if available)
{json.dumps(context.get('aggregations', {}), indent=2)[:2000]}

---

## USER QUESTION
{question}

---

## INSTRUCTIONS
1. Answer the question directly with specific numbers and data
2. If asking for statistics, provide all relevant stats (min, max, mean, sum, etc.)
3. If asking about categories/products, list them with their values
4. If asking about trends or patterns, explain what the data shows
5. Use markdown formatting for clarity (bold for numbers, tables where helpful)
6. Always base your answer on the actual data provided above
7. If the data doesn't contain information to answer the question, explain what data is available

Provide a comprehensive, helpful answer:"""

        response = model.generate_content(prompt)
        answer = response.text
        
        # Extract insights
        insights = []
        for col in context['columns']:
            if 'mean' in col:
                insights.append(f"{col['name']}: avg {col['mean']}, range {col['min']}-{col['max']}")
        
        return answer, insights[:3]
        
    except Exception as e:
        print(f"LLM analysis failed: {e}")
        # Fallback to comprehensive statistical analysis
        return generate_statistical_answer(df, question, context), []


def generate_statistical_answer(df, question, context):
    """Generate a statistical answer without LLM"""
    import pandas as pd
    
    question_lower = question.lower()
    
    # Build comprehensive stats
    answer = f"## Dataset Statistics\n\n"
    answer += f"**Overview:** {context['shape']['rows']:,} rows Ã— {context['shape']['columns']} columns\n\n"
    
    # Numeric columns stats
    answer += "### Numeric Columns\n"
    for col in context['columns']:
        if 'mean' in col:
            answer += f"**{col['name']}:**\n"
            answer += f"- Min: {col['min']}, Max: {col['max']}\n"
            answer += f"- Mean: {col['mean']}, Median: {col['median']}\n"
            answer += f"- Sum: {col['sum']}, Std Dev: {col['std']}\n\n"
    
    # Categorical columns
    answer += "### Categorical Columns\n"
    for col in context['columns']:
        if 'top_values' in col:
            answer += f"**{col['name']}** ({col['unique_count']} unique values):\n"
            for val, count in list(col['top_values'].items())[:5]:
                answer += f"- {val}: {count} occurrences\n"
            answer += "\n"
    
    return answer







def generate_basic_analysis(dataset, question):
    """Generate basic analysis without LLM"""
    question_lower = question.lower()
    
    if 'column' in question_lower or 'columns' in question_lower:
        cols = dataset.column_info.keys() if dataset.column_info else []
        return f"This dataset has {len(cols)} columns: {', '.join(cols)}"
    
    if 'row' in question_lower:
        return f"This dataset has {dataset.num_rows:,} rows."
    
    if 'missing' in question_lower or 'null' in question_lower:
        if dataset.column_info:
            missing = {k: v.get('null_count', 0) for k, v in dataset.column_info.items() if v.get('null_count', 0) > 0}
            if missing:
                return f"Missing values found in: {json.dumps(missing)}"
            return "No missing values found in the dataset."
        return "Column information not available."
    
    return f"Your dataset '{dataset.name}' has {dataset.num_rows:,} rows and {dataset.num_columns} columns. You can ask about:\n- Minimum/maximum values\n- Unique/different values in a column\n- Averages and statistics\n- Trends over time\n- Value counts"





@advanced_bp.route('/synthetic/suggest-schema', methods=['POST'])
@jwt_required()
def suggest_schema():
    """
    AI-powered schema suggestion from natural language description
    User describes what data they need, AI suggests a schema they can edit
    """
    data = request.get_json()
    description = data.get('description', '')
    
    if not description:
        return jsonify({'error': 'Description is required'}), 400
    
    try:
        # Try using LLM for smart schema generation
        schema = generate_schema_with_llm(description)
    except Exception as e:
        print(f"LLM schema generation failed: {e}")
        # Fallback to heuristic-based schema generation
        schema = generate_schema_heuristic(description)
    
    return jsonify({
        'success': True,
        'schema': schema,
        'description': description
    }), 200


def generate_schema_with_llm(description):
    """Use LLM to generate schema from description"""
    import google.generativeai as genai
    
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        raise Exception("No API key available")
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    prompt = f"""Analyze this dataset description and generate a JSON schema for synthetic data generation.

Description: "{description}"

Generate a JSON array of column definitions. Each column should have:
- "name": column name (snake_case)
- "type": one of ["id", "name", "email", "phone", "integer", "float", "category", "date", "boolean", "uuid", "address", "company"]
- For "integer"/"float": include "min" and "max"
- For "category": include "values" array with realistic options
- For "date": include "range_days" (how many days in the past)

Return ONLY valid JSON array, no markdown, no explanation. Example:
[
  {{"name": "customer_id", "type": "uuid"}},
  {{"name": "full_name", "type": "name"}},
  {{"name": "age", "type": "integer", "min": 18, "max": 80}},
  {{"name": "status", "type": "category", "values": ["active", "inactive", "pending"]}}
]"""

    response = model.generate_content(prompt)
    
    # Parse JSON from response
    response_text = response.text.strip()
    # Clean up any markdown formatting
    if response_text.startswith('```'):
        response_text = response_text.split('```')[1]
        if response_text.startswith('json'):
            response_text = response_text[4:]
    response_text = response_text.strip()
    
    schema = json.loads(response_text)
    return schema


def generate_schema_heuristic(description):
    """Generate schema using keyword heuristics"""
    columns = []
    desc_lower = description.lower()
    
    # Common dataset patterns
    patterns = {
        'customer': [
            {'name': 'customer_id', 'type': 'uuid'},
            {'name': 'name', 'type': 'name'},
            {'name': 'email', 'type': 'email'},
            {'name': 'phone', 'type': 'phone'},
            {'name': 'age', 'type': 'integer', 'min': 18, 'max': 80},
            {'name': 'city', 'type': 'category', 'values': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Miami', 'Seattle', 'Boston']},
        ],
        'sales': [
            {'name': 'transaction_id', 'type': 'uuid'},
            {'name': 'product_name', 'type': 'category', 'values': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor', 'Keyboard', 'Mouse', 'Charger']},
            {'name': 'quantity_sold', 'type': 'integer', 'min': 1, 'max': 100},
            {'name': 'unit_price', 'type': 'float', 'min': 10, 'max': 2000},
            {'name': 'sale_date', 'type': 'date', 'range_days': 365},
        ],
        'grocery': [
            {'name': 'product_id', 'type': 'id'},
            {'name': 'product_name', 'type': 'category', 'values': ['Milk', 'Bread', 'Eggs', 'Rice', 'Pasta', 'Vegetables', 'Fruits', 'Cheese', 'Yogurt', 'Juice']},
            {'name': 'quantity_sold', 'type': 'integer', 'min': 1, 'max': 50},
            {'name': 'unit_price', 'type': 'float', 'min': 1, 'max': 100},
            {'name': 'sale_date', 'type': 'date', 'range_days': 90},
            {'name': 'store_location', 'type': 'category', 'values': ['Downtown', 'Mall', 'Suburb', 'Airport', 'Station']},
        ],
        'employee': [
            {'name': 'employee_id', 'type': 'uuid'},
            {'name': 'name', 'type': 'name'},
            {'name': 'email', 'type': 'email'},
            {'name': 'department', 'type': 'category', 'values': ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', 'Operations']},
            {'name': 'salary', 'type': 'float', 'min': 30000, 'max': 150000},
            {'name': 'hire_date', 'type': 'date', 'range_days': 1825},
        ],
        'product': [
            {'name': 'product_id', 'type': 'id'},
            {'name': 'product_name', 'type': 'category', 'values': ['Widget A', 'Widget B', 'Gadget X', 'Gadget Y', 'Tool Pro', 'Tool Basic']},
            {'name': 'category', 'type': 'category', 'values': ['Electronics', 'Home', 'Office', 'Sports', 'Fashion']},
            {'name': 'price', 'type': 'float', 'min': 5, 'max': 500},
            {'name': 'stock_quantity', 'type': 'integer', 'min': 0, 'max': 1000},
        ],
        'order': [
            {'name': 'order_id', 'type': 'uuid'},
            {'name': 'customer_name', 'type': 'name'},
            {'name': 'order_date', 'type': 'date', 'range_days': 180},
            {'name': 'total_amount', 'type': 'float', 'min': 10, 'max': 5000},
            {'name': 'status', 'type': 'category', 'values': ['pending', 'processing', 'shipped', 'delivered', 'cancelled']},
        ],
    }
    
    # Check for pattern matches
    for keyword, schema in patterns.items():
        if keyword in desc_lower:
            columns = schema.copy()
            break
    
    # If no pattern matched, build from individual keywords
    if not columns:
        if 'id' in desc_lower or 'uuid' in desc_lower:
            columns.append({'name': 'id', 'type': 'uuid'})
        if 'name' in desc_lower:
            columns.append({'name': 'name', 'type': 'name'})
        if 'email' in desc_lower:
            columns.append({'name': 'email', 'type': 'email'})
        if 'phone' in desc_lower:
            columns.append({'name': 'phone', 'type': 'phone'})
        if 'age' in desc_lower:
            columns.append({'name': 'age', 'type': 'integer', 'min': 18, 'max': 80})
        if 'city' in desc_lower or 'location' in desc_lower:
            columns.append({'name': 'city', 'type': 'category', 'values': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']})
        if 'price' in desc_lower or 'amount' in desc_lower:
            columns.append({'name': 'amount', 'type': 'float', 'min': 10, 'max': 1000})
        if 'date' in desc_lower:
            columns.append({'name': 'date', 'type': 'date', 'range_days': 365})
        if 'quantity' in desc_lower:
            columns.append({'name': 'quantity', 'type': 'integer', 'min': 1, 'max': 100})
    
    # Default fallback
    if not columns:
        columns = [
            {'name': 'id', 'type': 'id'},
            {'name': 'name', 'type': 'name'},
            {'name': 'value', 'type': 'float', 'min': 0, 'max': 100},
            {'name': 'category', 'type': 'category', 'values': ['A', 'B', 'C']},
            {'name': 'created_at', 'type': 'date', 'range_days': 365}
        ]
    
    return columns


@advanced_bp.route('/synthetic/generate', methods=['POST'])
@jwt_required()
def generate_synthetic_data():
    """
    Generate Synthetic Data using LLM/Statistical Methods
    Accepts either custom_schema (text) or parsed_schema (JSON array from suggest-schema)
    """
    user_id = int(get_jwt_identity())
    data = request.get_json()
    
    mode = data.get('mode', 'from-schema')
    dataset_id = data.get('dataset_id')
    num_rows = min(data.get('num_rows', 1000), 100000)
    quality = data.get('quality', 'balanced')
    preserve_distribution = data.get('preserve_distribution', True)
    add_noise = data.get('add_noise', False)
    noise_level = data.get('noise_level', 5)
    custom_schema = data.get('custom_schema')
    parsed_schema = data.get('parsed_schema')  # Pre-parsed schema from suggest-schema
    
    try:
        if mode == 'custom' and parsed_schema:
            # Generate from pre-defined JSON schema (from suggest-schema endpoint)
            result = generate_from_parsed_schema(parsed_schema, num_rows, quality)
        elif mode == 'custom' and custom_schema:
            # Generate from natural language description
            result = generate_from_description(custom_schema, num_rows, quality)
        elif dataset_id:
            # Generate from existing dataset
            dataset = Dataset.query.filter_by(id=dataset_id, user_id=user_id).first()
            if not dataset:
                return jsonify({'error': 'Dataset not found'}), 404
            
            result = generate_from_dataset(dataset, num_rows, quality, preserve_distribution, add_noise, noise_level)
        else:
            return jsonify({'error': 'Either dataset_id, custom_schema, or parsed_schema required'}), 400
        
        return jsonify(result), 200
        
    except Exception as e:
        print(f"Synthetic data generation failed: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


def generate_from_parsed_schema(schema, num_rows, quality):
    """Generate synthetic data from pre-defined JSON schema"""
    import numpy as np
    from datetime import datetime, timedelta
    import random
    import uuid as uuid_module
    
    # Generate data for each column
    data = {}
    for col in schema:
        col_type = col.get('type', 'string')
        col_name = col.get('name', 'column')
        
        if col_type == 'id':
            data[col_name] = list(range(1, num_rows + 1))
        
        elif col_type == 'uuid':
            data[col_name] = [str(uuid_module.uuid4()) for _ in range(num_rows)]
        
        elif col_type == 'name':
            first_names = ['James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael', 'Linda', 'William', 'Elizabeth', 'David', 'Susan', 'Richard', 'Karen', 'Joseph']
            last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez', 'Wilson', 'Anderson', 'Taylor', 'Thomas']
            data[col_name] = [f"{random.choice(first_names)} {random.choice(last_names)}" for _ in range(num_rows)]
        
        elif col_type == 'email':
            domains = ['gmail.com', 'yahoo.com', 'outlook.com', 'company.com', 'example.org']
            data[col_name] = [f"user{i}@{random.choice(domains)}" for i in range(num_rows)]
        
        elif col_type == 'phone':
            data[col_name] = [f"+1-{random.randint(200,999)}-{random.randint(100,999)}-{random.randint(1000,9999)}" for _ in range(num_rows)]
        
        elif col_type == 'integer':
            min_val = col.get('min', 0)
            max_val = col.get('max', 100)
            data[col_name] = np.random.randint(min_val, max_val + 1, size=num_rows).tolist()
        
        elif col_type == 'float':
            min_val = col.get('min', 0)
            max_val = col.get('max', 100)
            data[col_name] = np.round(np.random.uniform(min_val, max_val, size=num_rows), 2).tolist()
        
        elif col_type == 'category':
            values = col.get('values', ['A', 'B', 'C'])
            data[col_name] = [random.choice(values) for _ in range(num_rows)]
        
        elif col_type == 'date':
            base = datetime.now()
            range_days = col.get('range_days', 365)
            data[col_name] = [(base - timedelta(days=random.randint(0, range_days))).strftime('%Y-%m-%d') for _ in range(num_rows)]
        
        elif col_type == 'boolean':
            data[col_name] = [random.choice([True, False]) for _ in range(num_rows)]
        
        elif col_type == 'address':
            streets = ['Main St', 'Oak Ave', 'Park Blvd', 'First St', 'Market St', 'Broadway', 'Elm St']
            cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Seattle', 'Miami']
            data[col_name] = [f"{random.randint(100,9999)} {random.choice(streets)}, {random.choice(cities)}" for _ in range(num_rows)]
        
        elif col_type == 'company':
            companies = ['Acme Corp', 'TechStart Inc', 'Global Solutions', 'DataFlow LLC', 'CloudNine Systems', 'Innovate Labs', 'NextGen Tech']
            data[col_name] = [random.choice(companies) for _ in range(num_rows)]
        
        else:
            data[col_name] = [f"value_{i}" for i in range(num_rows)]
    
    df = pd.DataFrame(data)
    
    # Create preview
    preview = {
        'columns': list(df.columns),
        'rows': df.head(10).values.tolist()
    }
    
    return {
        'success': True,
        'rows_generated': num_rows,
        'columns_count': len(schema),
        'quality_score': 95 if quality == 'high' else 88 if quality == 'balanced' else 80,
        'preview': preview,
        'download_url': None
    }


def generate_from_description(description, num_rows, quality):
    """Generate synthetic data from natural language description"""
    import numpy as np
    from datetime import datetime, timedelta
    import random
    
    # Parse description with LLM or use heuristics
    columns = parse_schema_description(description)
    
    # Generate data for each column
    data = {}
    for col in columns:
        data[col['name']] = generate_column_data(col, num_rows)
    
    df = pd.DataFrame(data)
    
    # Create preview
    preview = {
        'columns': list(df.columns),
        'rows': df.head(10).values.tolist()
    }
    
    return {
        'success': True,
        'rows_generated': num_rows,
        'columns_count': len(columns),
        'quality_score': 92 if quality == 'high' else 85 if quality == 'balanced' else 78,
        'preview': preview,
        'download_url': None  # Would generate actual download URL
    }


def parse_schema_description(description):
    """Parse natural language schema description"""
    # Default columns based on common patterns
    columns = []
    desc_lower = description.lower()
    
    if 'name' in desc_lower:
        columns.append({'name': 'name', 'type': 'name'})
    if 'email' in desc_lower:
        columns.append({'name': 'email', 'type': 'email'})
    if 'age' in desc_lower:
        columns.append({'name': 'age', 'type': 'integer', 'min': 18, 'max': 65})
    if 'city' in desc_lower or 'location' in desc_lower:
        columns.append({'name': 'city', 'type': 'category', 'values': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego']})
    if 'amount' in desc_lower or 'price' in desc_lower or 'purchase' in desc_lower:
        columns.append({'name': 'amount', 'type': 'float', 'min': 10, 'max': 500})
    if 'date' in desc_lower:
        columns.append({'name': 'date', 'type': 'date', 'range_days': 730})
    if 'id' in desc_lower:
        columns.append({'name': 'id', 'type': 'id'})
    
    # Add default columns if none detected
    if not columns:
        columns = [
            {'name': 'id', 'type': 'id'},
            {'name': 'value', 'type': 'float', 'min': 0, 'max': 100},
            {'name': 'category', 'type': 'category', 'values': ['A', 'B', 'C']},
            {'name': 'timestamp', 'type': 'date', 'range_days': 365}
        ]
    
    return columns


def generate_column_data(col, num_rows):
    """Generate data for a single column"""
    import numpy as np
    from datetime import datetime, timedelta
    import random
    
    col_type = col.get('type', 'string')
    
    if col_type == 'id':
        return list(range(1, num_rows + 1))
    
    elif col_type == 'name':
        first_names = ['James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael', 'Linda', 'William', 'Elizabeth']
        last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez']
        return [f"{random.choice(first_names)} {random.choice(last_names)}" for _ in range(num_rows)]
    
    elif col_type == 'email':
        domains = ['gmail.com', 'yahoo.com', 'outlook.com', 'company.com']
        return [f"user{i}@{random.choice(domains)}" for i in range(num_rows)]
    
    elif col_type == 'integer':
        return np.random.randint(col.get('min', 0), col.get('max', 100), size=num_rows).tolist()
    
    elif col_type == 'float':
        return np.round(np.random.uniform(col.get('min', 0), col.get('max', 100), size=num_rows), 2).tolist()
    
    elif col_type == 'category':
        values = col.get('values', ['A', 'B', 'C'])
        return [random.choice(values) for _ in range(num_rows)]
    
    elif col_type == 'date':
        base = datetime.now()
        range_days = col.get('range_days', 365)
        return [(base - timedelta(days=random.randint(0, range_days))).strftime('%Y-%m-%d') for _ in range(num_rows)]
    
    else:
        return [f"value_{i}" for i in range(num_rows)]


def generate_from_dataset(dataset, num_rows, quality, preserve_distribution, add_noise, noise_level):
    """Generate synthetic data based on existing dataset"""
    import numpy as np
    import io
    from app.services.minio_service import get_minio_service
    
    try:
        minio_service = get_minio_service()
        file_content = minio_service.download_bytes('datasets', dataset.file_path)
        
        if dataset.file_type == 'csv':
            df = pd.read_csv(io.BytesIO(file_content))
        else:
            df = pd.read_excel(io.BytesIO(file_content))
    except:
        # Fallback: generate from schema
        if dataset.column_info:
            columns = [{'name': col, 'type': 'float' if 'float' in str(info.get('dtype', '')) else 'category'} 
                      for col, info in dataset.column_info.items()]
            return generate_from_description(str(columns), num_rows, quality)
        raise Exception("Could not load dataset")
    
    # Generate synthetic data preserving distributions
    synthetic_data = {}
    
    for col in df.columns:
        if df[col].dtype in ['int64', 'int32', 'float64', 'float32']:
            if preserve_distribution:
                # Sample from same distribution
                mean = df[col].mean()
                std = df[col].std()
                synthetic_data[col] = np.random.normal(mean, std, num_rows)
                if df[col].dtype in ['int64', 'int32']:
                    synthetic_data[col] = np.round(synthetic_data[col]).astype(int)
            else:
                synthetic_data[col] = np.random.uniform(df[col].min(), df[col].max(), num_rows)
        else:
            # Categorical - preserve frequency
            value_counts = df[col].value_counts(normalize=True)
            synthetic_data[col] = np.random.choice(
                value_counts.index.tolist(),
                size=num_rows,
                p=value_counts.values.tolist()
            )
    
    # Add noise if requested
    if add_noise:
        for col in synthetic_data:
            if isinstance(synthetic_data[col][0], (int, float)):
                noise = np.random.normal(0, noise_level / 100 * np.std(synthetic_data[col]), num_rows)
                synthetic_data[col] = synthetic_data[col] + noise
    
    synthetic_df = pd.DataFrame(synthetic_data)
    
    preview = {
        'columns': list(synthetic_df.columns),
        'rows': synthetic_df.head(10).values.tolist()
    }
    
    return {
        'success': True,
        'rows_generated': num_rows,
        'columns_count': len(synthetic_df.columns),
        'quality_score': 94 if quality == 'high' else 87 if quality == 'balanced' else 80,
        'preview': preview,
        'download_url': None
    }



